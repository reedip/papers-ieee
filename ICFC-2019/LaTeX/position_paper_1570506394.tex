\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{url}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Edge Clouds Control Plane and Management Data Consistency
Challenges: Position Paper for IEEE International Conference on Cloud
Engineering, 2019\\ }

\author{\IEEEauthorblockN{Bohdan Dobrelia}
\IEEEauthorblockA{\textit{OpenStack platform} \\
\textit{Red Hat}\\
Poznan, Poland \\
bdobreli@redhat.com}
}

\maketitle

\begin{abstract}
Fog computing is emerging Cloud of (Edge) Clouds technology. Its control plane
and deployments data synchronization is a major challenge. Autonomy requirements
expect even the most distant edge sites always manageable, available for
monitoring and alerting, scaling up/down, upgrading and applying security fixes.
Whenever temporary disconnected sites are managed locally or centrally, some
changes and data need to be eventually synchronized back to the central site(s)
with having its merge-conflicts resolved for the central data hub(s). While
some data needs to be pushed from the central site(s) to the Edge, which might
require resolving data collisions at the remote sites as well. In this paper,
we position the outstanding data synchronization problems for OpenStack
platform becoming a cloud solution number one for fog computing. We define the
inter-cloud operational invariants based on that Always Available autonomy
requirement. We show that a causally consistent data storage is the best
match for the outlined operational invariants and there is a great opportunity
for designing such a solution for Edge clouds. Finally, the paper brings the
vision of unified tooling to solve the data synchronization problems the same
way for infrastructure owners, IaaS cloud operators and tenants running
workloads for PaaS, like OpenShift or Kubernetes deployed on top of Edge
clouds.
\end{abstract}

\begin{IEEEkeywords}
Open source software, Edge computing, Distributed computing, System
availability, Design
\end{IEEEkeywords}

\section{Introduction}

OpenStack is an Infrastructure-as-a-Service platform number one for private
cloud computing, and it becomes being so for fog computing as well.
Hybridization and Mutli-cloud trends for private clouds interconnected with
public clouds and Platform-as-a-Service (PaaS) solutions, like
OpenShift/Kubernetes, allow the containerization of micro-services oriented
workloads to emerge in a highly portable, self-contained and the hosting
cloud-agnostic way. Giving it massively distributed scale of fog computing and
bringing the data it operates closer to end users, opens great opportunities
for Internet of Things (IoT) and nextgen global telecommunication technologies,
which first of all requires low-latency and higlhy responsive interfaces always
available for end users.

Speaking of always available, back to the system administration realities, the
Edge clouds control and management plane capabilities in such a perfect world
shall not fall behind as well. This paper is about to position the associated
data replication challenges and to bring vision of future development trends on
that topic, both for OpenStack and hopefully for anything residing on top of
it, e.g. PaaS and/or workloads designed for massively distributed scale and
following the IoT/fog computing best practices.

\section{Glossary}

Aside of the established terms\cite{b3}, we define a few more for the data
processing and operational aspects:

\subsection{Deployment Data}

Data that represents the configuration of \textit{cloudlets}\cite{b3}, like
API endpoints URI, or numbers of deployed \textit{edge nodes}\cite{b3} in
\textit{edge clouds}\cite{b3}. That data represents the most recent state of a
deployment.

\subsection{Cloud Data}

Represents the most recent\footnote{when there is unresolved data merging
conflicts, the most recent state becomes the best known state} internal and
publicly visible state of cloudlets, like cloud users or virtual routers. Cloud
data also includes logs, performance and usage statistics, state of message
queues and the contents of databases.

\subsection{Control Plane}

Corresponds to any operations performed via cloudlets API endpoints or command
line tooling. For example, starting a virtual machine instance, or creating a
cloud user. Such operations are typically initiated by cloud applications,
tenants or operators.

\subsection{Management Plane}

Corresponds to administrative actions performed via configuration and lifecycle
management systems. Such operations are typically targeted for cloudlets, like
edge nodes, \textit{edge data centers}\cite{b3}, or edge clouds. E.g.
upgrading or reconfiguring cloudlets in a \textit{virtual data center}
\cite{b3}, or scaling up edge nodes. And typically initiated by cloud
infrastructure owners. For some cases, like Baremetal-as-a-Service, tenants
may as well initiate actions executed via the management plane. Collecting
logs, performance and usage statistics for monitoring and alerting systems also
represents the management plane operations, although it operates with the cloud
data.

\subsection{Always Available}

The operational mode of the control and management planes that corresponds to
the \textit{sticky available}\cite{b4} causal consistency models,
i.e. \textit{Real-Time Causal}\cite{b2}, or \textit{causal+}\cite{b1}.

The stickiness property imposes an additional constraint\cite{b4}: ``on every
non-faulty node, so long as clients only talk to the same servers, instead of
switching to new ones''. And the real-time constraint is keeping the system
time synchronized for all cloudlets. Causal+ and Real-Time causal consistency
ensures ordering of relative operations, i.e. all causally related writes can
be seen in the same order by all processes (connected to the same server). All
that provides the best causal consistency guarantees\footnote{for
\textit{one-way convergent}\cite{b2} systems} we can get for today.

\section{Analysis and Discussion}

\subsection{Autonomy Requirements}

We define always available autonomy for cloudlets as the following strict
requirements:

\begin{itemize}
  \item any operations performed on cloudlets state\footnote{despite the
    cloudlets aliveness or failure conditions} fit data consistency models that
    allow the involved control/management planes operating as always available,
    and there is no read-only or blocking access limitations.
  \item cloudlets data can be modified at any given moment of time, despite of
    inter-cloudlets network connectivity\footnote{for disconnected/partitioned
    cloudlets, data can be modified via local control/management plane, if
    exists and not failed. Despite the adjacent \textit{aggregation edge
    layer}\cite{b3} global view and/or quorum requirements}.
  \item data can be synchronized eventually across cloudlets
    to/from\footnote{bi-directional data replication is not a strict
    requirement though. For some cases, it is acceptable to have state
    replicated only from aggregation edge layer to cloudlet(s) under its
    control/management, like virtual machine or hardware provisioning images
    data. Or otherwise, logs, performance and metering statistics can flow from
    cloudlets to aggregation edge layer. Real-Time Causal } its adjacent aggregation edge layer,
    but not horizontally\footnote{also implies there is no horizontal data
    replication across neighbor aggregation edge layers. This corresponds to
    the acyclic graph (tree) topology}.
  \item data replication conflicts can be resolved automatically or by hand,
    and/or queued\footnote{depending on the numbers and allowed isolation
    periods of cloudlets under control/management, the disc and memory
    requirements for aggregation edge layers may vary drastically} for later
    processing\footnote{COPS\cite{b1} supports a similar failure mode for a
    datacenter: ``the system administrator has two options: allow the queues to
    grow if the partition is likely to heal soon, or reconfigure COPS to no
    longer use the failed datacenter'', except that the datacenter may be
    either failed or operating isolated long-time, and that we can have
    multiple such datacenters}.
\end{itemize}

\subsection{Operational Invariants}

To be always available as we defined it, control and management planes of
cloudlets should provide the following operational capabilities
(\textit{invariants} hereafter):

\begin{itemize}
  \item TBD (see ./ICFC-2019/challenges.md)
\end{itemize}

\subsection{Data Consistency Requirements}

The operational invariants dictate inevitable presense of shared state and
sophisticated data replication mechanisms\footnote{when we refer to just
\textit{data} or \textit{state}, we do not differentiate either that is
deployment or control data} among cloudlets. We know\cite{b4} that
\textit{causally consistent}\cite{b1}\cite{b2} data accessing mechanisms is
the best fit for the declared invariants of the always available control and
management planes.

OpenStack and OpenShift/Kubernetes, have yet causally consistent data
backends\footnote{that is, for control/deployment data only} supported. The
vision of the unified architecture based on such an always available data
storage dictates us to not consider different backends for control and
deployment data. Although generic version control systems, like Git, might fit
all for the deployment data versioning, replicating and manual conflicts
resolving, that would break the unified design approach for cloud data
processing.

OpenStack cloud data is normally stored in databases via transactions based on
stronger than causal \textit{unavailable}\cite{b4} data consistency models, e.g.
\textit{serializable}\cite{b4}, or \textit{repeatable read}\cite{b4}.

The weaker than causal+ and Real-Time Causal \textit{total available}\cite{b4}
consistency models may be considered as an alternative. Transactional global
databases\cite{b5}, may technically support it\footnote{not Galera/MariaDB
cluster though, as it has a strict quorum requierements for database writes}. A
weaker consistency model provides a really poor alternative though as it brings
increased implementaion complexity, like corner cases handling for either the
storage replicas, or client sided, or both, associated with relaxed
constraints. E.g. \textit{monotonic atomic view}\cite{b4} does not impose any
real-time constraints, while Real-Time Causal does, which somewhat simplifies
the end system design. Additionally, monotonic atomic view would require
sophisticated handling of \textit{fuzzy reads}\cite{b4},
\textit{phantoms}\cite{b4}, discarded write-only transactions, empty state
returned for any reads. All that makes that the strongest option for totally
available data backends less preferable than causally (sticky) consistent
ones.

Kubernetes clusters state is backed with Etcd, which only supports the stronger
than Real-Time Causal consistency models.

\subsection{Vision of a Unified Control/Cloud Data Storage Design}

The definition we made for always available distributed systems self-explains
why the causally (sticky) consistent storage backends is the best match for the
cloudlets autonomy requirements and operational invariants as we defined those.

COPS formally proves implementation of a client library and highly scalable
tooling for causal+ data operations. By design, it does not impose any
real-time constraints and supports a single Edge data center failure. The real
tooling made off that base, may be operating on top of the nonshared local cloudlets
databases, or key value storages (KVS), that provide the stronger consistency
guarantees by the costs of reduced local availability\footnote{that is, the local view
for a cloudlet and have no impact onto global views}. That would
work as weaker consistency guarantees work well, when built on top of the
stronger ones, and provide an always available global view of cloudlets for the
adjacent aggregation edge layer that controls/manages these cloudlets as the
next hop connection. Replicating the state changes via causally related operations
and conflicts resolving via custom handlers is that COPS covers as well.

The open questions are:
\begin{itemize}
  \item Does COPS retains operations causal+ related when executed over
    multiple datacenters failure events (or extended time of being network
    partitioned)? Given the operational invariants, an aggregation edge layer
    cloudlet should allow all of its managed/controlled cloudlets running fully
    autonomous long-time, having all the outgoing operations queued and either
    eventually applied with conflicts resolved, or dropped/expired\footnote{the
    global view of fully autonomous cloudlets may be represented for the
    agregation edge layer with the state marks, like ``unknown/autonomous'',
    ``synchronizing'', ``connected'', ``failed/disconnected/fenced'', if and
    only if it is confirmed as failed, or manually disconnected, or fenced
    automatically}.
  \item Does COPS support two-way convergent systems, in terms of \cite{b2},
    for bi-directional causal+ replications?
\end{itemize}

TODO: find a use for Real-Time Causal and \cite{b6} alternatives to form more
options for vision. Finally, make preferences for causal databases vs KVS, if
possible?

\section{Conclusion}

TBD

\begin{thebibliography}{00}
\bibitem{b1} W. Lloyd, M. J. Freedman, M. Kaminsky, and D. G. Andersen, ``Donâ€™t settle for eventual: Scalable causal consistency for wide-area storage with COPS,'' Proc. 23rd ACM Symposium on Operating Systems Principles (SOSP 11), Cascais, Portugal, October 2011.
\bibitem{b2} P. Mahajan, L. Alvisi, and M. Dahlin. ``Consistency, availability, and convergence,'' Technical Report TR-11-22, Univ. Texas at Austin, Dept. Comp. Sci., 2011.
\bibitem{b3} The Linux Foundation, ``Open Glossary of Edge Computing,'' [Online]. Available: \url{https://github.com/State-of-the-Edge/glossary}
\bibitem{b4} K. Kingsbury, ``Consistency Models,'' [Online]. Available: \url{https://jepsen.io/consistency}
\bibitem{b5} M. Bayer, ``Global Galera Database,'' [Online]. Available: \url{https://review.openstack.org/600555}
\bibitem{b6} M. M. Elbushra, J. Lindstrom, ``Causal Consistent Databases'', Open Journal of Databases (OJDB), Volume 2, Issue 1, 2015.
\end{thebibliography}
\end{document}
