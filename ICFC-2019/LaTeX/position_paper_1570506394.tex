\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{url}
\usepackage[linguistics]{forest}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Position Paper: Edge Clouds Control Plane and Management Data
Replication Challenges\\
}

\author{\IEEEauthorblockN{1\textsuperscript{st} Bohdan Dobrelia}
\IEEEauthorblockA{\textit{OpenStack platform} \\
\textit{Red Hat}\\
Poznan, Poland \\
bdobreli@redhat.com}
\and
\IEEEauthorblockN{2\textsuperscript{nd} Given Name Surname}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address}
\and
\IEEEauthorblockN{3\textsuperscript{rd} Given Name Surname}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address}
}

\maketitle

\begin{abstract}
Fog computing is an emerging paradigm aiming at bringing cloud functions closer
to the end users and data sources. Its control plane and deployments data
synchronization is a major challenge. Autonomy requirements expect even the
most distant edge sites always manageable, available for monitoring and
alerting, scaling up/down, upgrading and applying security fixes. Whenever
temporary disconnected sites are managed locally or centrally, some changes
and data need to be eventually synchronized back to the central site(s) with
having its merge-conflicts resolved for the central data hub(s). While some
data needs to be pushed from the central site(s) to the Edge, which might
require resolving data collisions at the remote sites as well. In this paper,
we position the outstanding data synchronization problems for OpenStack
platform becoming a cloud solution number one for fog computing. We define
the inter-cloud operational invariants based on that Always Available
autonomy requirement.  We show that a causal consistent data replication is
the best match for the outlined operational invariants and there is a great
opportunity for designing such a solution for Edge clouds. Finally, the paper
brings vision of unified tooling to solve outstanding state synchronization
problems the same way for infrastructure owners, cloud operators and tenants
running stateful workloads hosted on OpenStack IaaS or OpenShift/Kubernetes
PaaS deployed in Edge clouds as multi-cloud workloads abstraction and
unification layer, to make it truly cloud-vendors agnostic and portable.
\end{abstract}

\begin{IEEEkeywords}
Open source software, Edge computing, Distributed computing, System
availability, Design
\end{IEEEkeywords}

\section{Introduction}

OpenStack is an Infrastructure-as-a-Service platform number one for private
cloud computing, and it becomes being so for fog computing as well.
Hybridization and multi-cloud trends for private clouds interconnected with
public clouds and Platform-as-a-Service (PaaS) solutions, like
OpenShift/Kubernetes, allow the containerization of micro-services oriented
workloads to emerge in a highly portable, self-contained and the hosting
cloud-agnostic way. Giving it massively distributed scale of fog computing and
bringing the data it operates closer to end users, opens great opportunities
for Internet of Things (IoT) and next-gen global telecommunication technologies,
which first of all requires low-latency and highly responsive interfaces always
available for end users.

Speaking of always available, back to the system administration realities, the
Edge clouds control and management plane capabilities in such a massively
distributed world shall not fall behind as well. This paper positions
challenges associated with replicating state, like cloud or deployment data
and/or operations, and defines minimum viable operational capabilities to have
it always available as the best effort. It finally brings vision of unified
systems design approach for future data replication tooling to be implemented
for or integrated into OpenStack IaaS, OpenShift/Kubernetes PaaS that may
optionally reside on top of it, or as a separate compute platform, and stateful
workloads designed for massively distributed scale, which is natural for Edge
computing and IoT.

\section{Glossary}

Aside of the established terms\cite{b3}, we define a few more for the data
processing and operational aspects:

\textbf{Deployment Data}: data that represents the configuration of
\textit{cloudlets}\cite{b3}, like API endpoints URI, or numbers of deployed
\textit{edge nodes}\cite{b3} in \textit{edge clouds}\cite{b3}. That data may
represent either the most recent state\footnote{when there is unresolved data
merging conflicts or queued operations pending for future execution, the most
recent state becomes the best known state}of a deployment, or arbitrary data
chunks/operations required to alter that state.

\textbf{Cloud Data}: similarly to deployment data, represents the most recent
or the best known internal and publicly visible state of cloudlets, like cloud users or
virtual routers. Cloud data also includes logs, performance and usage
statistics, state of message queues and the contents of databases. It may as
well represent either data chunks or operations targeted for some state
$\mathrm{S}$ transitioning into a new state $\mathrm{S'}$.

\textbf{Control Plane}: corresponds to any operations performed via cloudlets
API endpoints or command line tooling. For example, starting a virtual machine
instance or a Kubernetes pod, or creating an OpenStack Keystone user. Such
operations are typically initiated by cloud applications, tenants or operators.

\textbf{Replication Topology}: represents hierarchy for allowed data
replication flows for interconnected cloudlets, including \textit{edge
aggregation layers}\cite{b3}. When we refer to an edge aggregation layer and
cloudlets under its control, we mean exactly any operations executed via the
control plane of that edge aggregation layer and targeted for cloudlets sitting
the next hop (which is adjacent up or down on the connections graph), or going
deeper through the nested graph connections). If the next hop is represented by
another edge aggregation layer, the same applies to its adjacent cloudlets.

\begin{figure}[htbp]
\caption{Example Replication Topology}.
\begin{forest}
  [$\mathrm{EAL1}$
    [\textit{access edge layer}\cite{b3}
     [\textit{infrastructure edge}\cite{b3}
       [\textit{device edge}\cite{b3}]
     ]
    ]
    [$\mathrm{EAL2}$
      [$\mathrm{EAL3}$
        [$\mathrm{cloudlet1}$
        ]
      ]
      [$\mathrm{EAL4}$
        [$\mathrm{cloudlet2}$
          [$\mathrm{EAL5}$
            [$\mathrm{cloudlet3}$
          ]
          [$\mathrm{cloudlet4}$
          ]
        ]
      ]
    ]
  ]
]
\label{fig}
\end{forest}
\end{figure}

So effectively there is only a limitation for horizontally interconnected
cloudlets cannot be replicating its state to each other. This corresponds to
the acyclic graph (tree) topology.

For the example graph (Fig.~\ref{fig}), the $\mathrm{infrastructure\ edge}$ can
replicate data, like contributing its local stats into the global view of
hardware and performance metrics, to the the edge aggregation layer
$\mathrm{EAL1}$ omitting the $\mathrm{access\ edge\ layer}$. Meanwhile the
latter can be pushing something, like deployment data changes, to the
$\mathrm{device\ edge}$ and $\mathrm{infrastructure\ edge}$. On the right side
of the graph, $\mathrm{cloudlet3}$ and $\mathrm{cloudlet4}$ cannot replicate to
each other, but can do to the edge aggregation layer $\mathrm{EAL1}$ either
directly or consequently via $\mathrm{EAL5}$, $\mathrm{cloudlet2}$,
$\mathrm{EAL4}$ and $\mathrm{EAL2}$, which is totally the replication
implementation specific. We can also see that $\mathrm{EAL4}$ is
hierarchically associated with $\mathrm{EAL2}$ and $\mathrm{EAL1}$, which is
under control and/or management of either of two. But it is not associated with
$\mathrm{EAL3}$, $\mathrm{cloudlet2}$ or anything sitting down of two.

\textbf{Management Plane}: corresponds to administrative actions performed via
configuration and lifecycle management systems. Such operations are typically
targeted for cloudlets, like edge nodes, \textit{edge data centers}\cite{b3},
or edge clouds. E.g. upgrading or reconfiguring cloudlets in a \textit{virtual
data center}\cite{b3}, or scaling up edge nodes. And typically initiated by
cloud infrastructure owners. For some cases, like Baremetal-as-a-Service,
tenants may as well initiate actions executed via the management plane.
Collecting logs, performance and usage statistics for monitoring and alerting
systems also represents the management plane operations, although it operates
with the cloud data. When we refer to an \textit{edge aggregation
layer}\cite{b3} and cloudlets under its management, we mean exactly
administrative/deployment related operations executed via the management plane.
Similarly to the control plane, we impose the same data replication topology
without nesting limitations but restricted only for horizontal graph
connections.

\textbf{Data Replication Conflict}: according to\cite{b1}, two operations on
the same target are in conflict if they are not related by causality.

\textbf{Always Available}: the operational mode of the control and management
planes that corresponds to \textit{sticky available causal
consistency}\cite{b4} data replication models, i.e. RTC (\textit{Real-Time
Causal}\cite{b2}), or \textit{causal+}\cite{b1}. Depending on the consistency
model choices, there may be additional constraints:

\begin{itemize}
  \item the stickiness property is a mandatory constraint for sticky available
    causal consistent systems. That is: ``on every non-faulty node, so long
    as clients only talk to the same servers, instead of switching to new
    ones''\cite{b4}.

  \item the real-time constraint is keeping the system time synchronized for
    all cloudlets. That is a mandatory constraint for RTC.

  \item \textit{one-way convergence}\cite{b2} is a mandatory for RTC.
\end{itemize}

Causal+ and RTC consistency ensures ordering of relative operations, i.e. all
causal related writes can be seen in the same order by all processes
(connected to the same server). It is also known that ``the causal consistency
supports non-blocking operations; i.e. processes may complete read or write
operations without waiting for global computation. Therefore, the causal
consistency overcomes the primary limit of stronger criteria: communication
latency''\cite{b6}. All that provides the best causal consistency guarantees we
can get for today for always available systems.

\section{Analysis and Discussion}

\subsection{Autonomy Requirements}

We define always available autonomy for cloudlets as the following strict
requirements:

\subsubsection{Provide no read-only or blocking access for inter-cloudlet
operations}

Any operations performed on cloudlets state\footnote{despite the cloudlets
aliveness or failure conditions as it's shown in the global view of an adjacent
or hierarchically associated edge aggregation layer} fit consistency models
that allow the involved control/management planes operating as always
available, and there is no limitations, like read-only or blocking
access\footnote{operations may be queued for future processing in order to meet
this autonomy requirement}. Internal state of cloudlets though, is allowed to
keep its failure modes unchanged. E.g. for a local Galera/MariaDB database or
Etcd cluster, its standard quorum requirements apply for internal operations.
That is a transitioning requirement until the internal cloudlets state can be
migrated to causal consistent data storages as well. Operations may be queued
for future processing in order to meet this autonomy requirement.

\subsubsection{Provide a local control/management plane, whenever possible}

Operations on cloudlets can be scheduled at any given moment of time, despite
of inter-cloudlets network connectivity\footnote{for disconnected/partitioned
cloudlets, data can be modified via local control/management plane, if it
exists and not failed. Despite the quorum requirements of its hierarchically
associated aggregation edge layers}. The same transitioning requirements for
internal cloudlets state applies.

\subsubsection{Support fully autonomous or disconnected cloudlets}

Aggregation edge layer cloudlets should allow for arbitrary or all of
its managed/controlled cloudlets running fully autonomous long-time or
indefinitly long, queuing any operations targeted for such cloudlets.
For a permanently diconnected cloudlet it may make more sense though to
detach it from its adjacent aggregation layer and/or reorganize its place
taken in the replication topology.

\subsubsection{Queue operations to keep it always available at best
effort}

Queued operations have to be eventually applied if/after the control/management
plane capabilities restored, or dropped (e.g. expired) otherwise. That poses a
lazy replication principle. If there is intermediate aggregation edge layers
down the way to the target of the queued operations, the queue may be shared
across each of the involved aggregation edge layers\footnote{or optionally,
queued operations may be distributed across not shared queues}. That should
reduce the associated memory and disc pressure for aggregation layers.

\subsubsection{Provide a gloval view for cloudlets aliveness state}

Global view of cloudlets needs to be periodically presented for at least one of
the hierarchically associated aggregation edge layers. For example, with the
state marks, like ``unknown'', or ``autonomous''; ``synchronizing'',
``connected''; ``failed'', or ``disconnected'', or ``fenced'', if and only if
it is confirmed as failed, or manually disconnected, or fenced automatically.
That poses the aliveness of the control/management planes principle.

\subsection{Operational Invariants}

To be always available as we defined it, control and management planes of
cloudlets should provide the following operational capabilities
(\textit{invariants} hereafter):

\subsubsection{Keep control planes always available at best effort}

CRUD (Create, Read, Update and Delete) operations on cloud data can always be
requested via API/CLI of local cloudlets or associated aggregation layers
sitting on top of the replication hierarchy. The same queuing requirements
apply as it is defined for the autonomy requirements.

\subsubsection{Do not wait for edge agregation layers control planes for local
operations}

Local CRUD operations for disconnected/autonomous cloudlets,
if its control plane exists\footnote{a cloudlet may be running only
compute/storage resources. For such a case it cannot meet the always available
requirements, when disconnected/partitioned from its remote control plane} and
is not failed, can be processed without waiting for the upper aggregation
layers to recover its control over the cloudlets.

\subsubsection{Allow local scaling of infrastructure edge nodes without waiting
for management planes of edge aggregation layers}

Similarly the to control plane operations, deployed infrastructure edge nodes
can always be scheduled for scaling up/down by the cloudlets local management
planes, if it is possible\footnote{a cloudlet may be relying on the remote
configuration management only}, or via its hierarchically associated
aggregation layers. Same queuing requirements apply.

\subsubsection{Allow hotfixes and kernel/software updates applied locally for
cloudlets}

Security patches and minor system updates, including kernel upgrades, can
always be scheduled for installation by the same meanings (via operations
issued locally or by the associated aggregation layers, including the same
queuing requirements).

\subsubsection{Allow major software versions upgrades applied locally for
cloudlets}

Similarly, major versions of system components, like OpenStack or
OpenShift/Kubernetes platforms, can be always scheduled for upgrades, using the
same meanings as above.

\subsubsection{Provide an extended global view for cloudlets}

Additionally to the aforementioned global view for cloudlets control/management
plane aliveness state marks, there needs to be a periodically updated global
view for each of the edge aggregation layers into its controlled/managed
cloudlets, at least the adjacent ones, for the key system administration
aspects, like hardware status, power management, systems state logging,
monitoring and alerting events, performance and metering statistics.

\subsection{State Replication Consistency Requirements}

The operational invariants dictate inevitable presence of shared state and
sophisticated state replication and conflicts resolving
mechanisms for cloudlets.

When we refer to just \textit{data} or \textit{state}, we intentionally do not
differentiate either that is deployment or cloud data, or queued API/CLI
operations, to be replicated for either of the management and control planes.
While the final results of such causal consistent replication may end up in
different not shared storage backends proposing stronger data consistency
models, or in a global database, the replication tooling must provide causal
consistency and ideally should operate as an abstraction layer. That poses the
\textbf{unified approach principle}, which allows such tooling to solve state
replication problems for IaaS, PaaS or end users consuming it as
Replication-as-a-Service.

As it follows from the defined always available autonomy requirements and
operational invariants, we define the following data replication requirements:

\subsubsection{Incorporate convergent conflict handling\cite{b1}}

Data replication conflicts can be resolved automatically, or by hand and
maintained as causal related. The conflicts resolving strategies and rules
should be customizable, like ``last writer wins'' or ``return them all''. After
the conflicts resolved, the data may be considered causal related, that is by
definition\cite{b1} of the data conflicts in eventually consistent systems.

\subsubsection{Prefer one-way convergence in replication topologies}

As far as the replication topology and queuing capabilities allow
that, causal related data can be replicated across cloudlets. Prefer one-way
convergence and avoid bidirectional replication whenever possible.

\subsubsection{Bidirectional replication is only a nice to have requirement}

Bidirectional (two-way convergence) data replication is not a strict
requirement but is nice to have. Indeed, some state needs to be replicated
one-way from aggregation edge layer to cloudlets under its control/management,
like virtual machine or hardware provisioning images data. While logs,
performance and metering statistics may be collected only from cloudlets to its
hierarchically associated upper aggregation edge layers.

OpenStack and OpenShift/Kubernetes, have yet support for neither global causal
consistent data backends\footnote{that is, exclusively for cloud/deployment
data storage}, nor client libraries that could drive replication of casual
related state. That poses an open opportunity for developers and system
architects.

OpenStack cloud data is normally stored in databases via
transactions based on stronger than causal \textit{unavailable}\cite{b4} data
consistency models, e.g. \textit{serializable}\cite{b4}, or \textit{repeatable
read}\cite{b4}.

The weaker than causal+ and RTC \textit{total available}\cite{b4}
consistency models may be considered as an alternative. Transactional global
databases\cite{b5}, may technically support it\footnote{not Galera/MariaDB
cluster though, as it has a strict quorum requirements for database writes}. A
weaker consistency model provides a really poor alternative though as it brings
increased implementation complexity, like corner cases handling for either the
storage replicas, or client sided, or both, associated with relaxed
constraints. E.g. \textit{monotonic atomic view}\cite{b4} does not impose any
real-time constraints, while RTC does, which somewhat simplifies
the end system design. Additionally, monotonic atomic view would require
sophisticated handling of \textit{fuzzy reads}\cite{b4},
\textit{phantoms}\cite{b4}, discarded write-only transactions, empty state
returned for any reads. All that makes that the strongest option for totally
available data backends less preferable than causal consistent ones.

OpenShift/Kubernetes clusters state is backed with Etcd, which only supports
the stronger than causal consistency models.

\subsection{Data Replication Challenges}

All that brings us to challenges that need to be addressed:
\begin{itemize}
  \item categorizing control/management planes operations\footnote{at very
    least, limited to those that are the subject for state replication and
    required to meet the defined operational invariants} and cloud/deployment
    data flows associated with such operations, and grouping those into
    particular replication schemes for a future implementation/integration of
    tooling. The groups of distinct replication schemes may be identified by
    multiple dimensions, like low/high communication latency, short/long time
    support for disconnected/autonomous cloudlets, one-way/two-way convergence
    based, global causal storage or client libraries implementation specific.
    And for the latter case, either it should allow replicating data at a
    database/KVS level or synchronizing in-flight/queued operations at API
    level, or both. And for which data storage and message queuing backends to
    support such replication mechanisms\footnote{Given the current OpenStack
    and Kubernetes architecture, that should be at least clustered
    MySQL/MariaDB Galera databases, Etcd KVS, and RabbitMQ message queue}. In
    the end, the final solution should not bring excessive operational
    overhead, like if maintaining all of the identified replication schemes
    simultaneously for a deployment, and require not too much of human care,
    but still meet the unified approach principle as we defined it earlier.
  \item designing strategies and rules for conflicts resolving based on picked
    for a deployment groups of replication schemes identified the previous step.
  \item picking/combining groups of replication schemes for a deployment wise.
    The choices should be totally driven by replication topology expectations
    and upper constraints defined for inevitable operational overhead.
  \item building efficient state replication topologies matching the picked
    groups of replication schemes, depending on workloads types and numbers of
    cloudlets managed/controlled by edge aggregation layers. E.g. distributing
    locally queued operations targeted for disconnected cloudlets by nesting
    more of the edge aggregation layers, might help to lower the hardware
    requirements for all of the involved edge aggregation layers and allow
    cloudlets to operate fully autonomous for extended time and/or over
    flaky/higher latency connections.
  \item strictly abiding the unified approach principle for Edge clouds architecture
    and workloads designed for it. A future replication solution should be
    targeted for control and management planes of OpenStack as a minimum viable
    product (MVP). With having it potentially extended to OpenShift/Kubernetes
    PaaS, and ideally, presented in cloud application catalogs as a
    Replication-as-a-Service solution for hybrid and multi-cloud fog computing
    workloads/applications.
\end{itemize}

\subsection{Vision of a Unified Deployment/Cloud State Replication Design}

The definition we made for always available distributed systems self-explains
why the causal consistent state replication is the best match for the
massively distributed cloudlets autonomy requirements and operational
invariants as we defined those.

The vision of the unified architecture based on such an always available state
replication also dictates us to not consider different solutions for cloud and
deployment data or operations. Although generic version control systems, like
Git, might fit all cases for deployment data replicating and
conflicts resolving, that would break the unified design approach for cloud
data/state replication.

Client libraries implementing causal consistent data replication and
customizable conflicts resolving rules may provide a unification layer for
different underlying databases/KVS (Key Value Storage). The replication will be
effectively acting as database/KVS-to-database/KVS data synchronization tooling
syncing data at a database level. The main benefit for such an approach is no a
global data storage needed. Instead, the underlying local to cloudlets data
storages may keep operating as is, share nothing and provide unavailable
consistency models stronger than causal consistency. And cloudlets may keep
using different solutions for its local data storages as far as the tooling
supports such backends.

Additionally, client libraries may replicate not only data but operations at an
API level\footnote{For OpenStack Nova, there had been an example for such an
API level replication, that is cells v1 protocol. But it had been deprecated as
real cells v1 deployments required constant human care and feeding
operationally} as well, i.e. resolve conflicting operations on-fly, then apply
the resulting causal related operations for its original targets, effectively
replicating changes at an API level. Operations queued by the control/management planes, including
those targeted for disconnected/autonomous cloudlets, may be also processed
that way.

COPS formally proves implementation of a client library and highly scalable
tooling for causal+ data operations. By design, it does not impose any
real-time constraints and supports a single edge data center failure. The real
tooling made off that base, may be operating on top of the not shared local
cloudlets databases, or KVS, that provide the stronger consistency guarantees
by the costs of reduced local availability\footnote{that is, the local view for
a cloudlet and have no impact onto global views}. That would work as weaker
consistency guarantees work well, when built on top of the stronger ones, and
provide an always available global view of cloudlets for hierarchically
associated upper aggregation edge layers. Replicating the state changes via
causal related operations and conflicts resolving via custom handlers is that
COPS covers as well.

Global causal consistent databases\cite{b6} is an alternative approach to
client libraries operating on top of cloudlets databases/KVS/API. The downside
is such a database has to be supported as a control/management planes data
backend for OpenStack/OpenShift/Kubernetes and/or any stateful cloud
applications leveraging such a global database as a Replication-as-a-Service
solution. And local cloudlets databases/KVS have to be switched to that global
database.

\begin{itemize}
  \item \textbf{Open questions}: does COPS retains operations causal+ related
    when executed over multiple data centers failure events (or extended time
    of being network partitioned?)

  \item \textbf{Open questions}: is COPS applicable for two-way convergent
    systems, in terms of\cite{b2}, for bidirectional causal+ replications?
    Given that\cite{b1} proves RTC provides the strongest causal consistency
    for one-way convergence only, and given that it provides stronger
    consistency than causal+, we can conclude that causal+ cannot provide the
    strongest causal consistency for bidirectional communication neither.

  \item \textbf{Open questions}: how much of all of the cloudlets data
    replication cases can be performed one-way? That would drastically simplify
    future implementation: ``Although most implementations use bidirectional
    communication, the communication from the update-receiver to the
    update-sender is just a (significant) performance optimization used to
    avoid redundant transfers of updates already known to the receiver. One-way
    convergence is also important in protocols that transmit updates via delay
    tolerant networks''\cite{b2}.

  \item \textbf{Open questions}: what solutions can we propose for operations
    that still do require a bidirectional data synchronization, like unique
    cloud users ID, without breaking the always availability autonomy
    requirements for the control and management planes? Global causal
    consistent databases may be a good choice for that. Alternatively, instead
    of bidirectionally replicating such data, inter-cloudlets API-to-API based
    synchronization mechanisms may become a solution.

  \item \textbf{Open questions}: which of the existing causal consistent
    databases\cite{b6} can be ingegrated as a global database/KVS solution that
    follows the unified approach for at least OpenStack and Kubernetes control
    and management planes, optionally replacing the storage for internal cloudlets
    state as well\footnote{to provide a decent alternative to Etcd and Galera clusters
    and maintain a unified global database over a cloud of Edge clouds
    instead}? The same causal consistent database may be made available for
    cloud applications via application catalogs.

  \item \textbf{Open questions}: which of the existing client library tooling
    for causal database or API level replication, like RTC or COPS, can be
    ingegrated for at least OpenStack and Kubernetes control/management planes
    to operate on top of Etcd and Galera clusters or endpoints API?
\end{itemize}

\section{Conclusion}

We defined autonomy requirements for the control and management planes of
massively distributed edge cloudlets and imposed minimum viable operational
invariants off those autonomy requirements. That brought us to consistency
requirements for cloudlets state replication and associated challenges.
Finally, we posed vision of key design principles, like queuing and lazy
replication, aliveness of the control and management planes and a unified
approach for the subject tooling. That is, state replication tooling to be
based on either global causal consistent databases, or client libraries that
replicate not shared local data at databases/KVS level. Or client libraries that
replicate operations at an API level.

We want to position the unification principle as the most important thing and
the greatest opportunity for developers to do it ``the right way'', which is to
bring the best of two IaaS and PaaS worlds for end users whom such data
replication tooling might benefit as-a-service, i.e. cloud tenants,
infrastructure owners and operators.

\begin{thebibliography}{00}
\bibitem{b1} W. Lloyd, M. J. Freedman, M. Kaminsky, and D. G. Andersen, ``Donâ€™t settle for eventual: Scalable causal consistency for wide-area storage with COPS,'' Proc. 23rd ACM Symposium on Operating Systems Principles (SOSP 11), Cascais, Portugal, October 2011.
\bibitem{b2} P. Mahajan, L. Alvisi, and M. Dahlin. ``Consistency, availability, and convergence,'' Technical Report TR-11-22, Univ. Texas at Austin, Dept. Comp. Sci., 2011.
\bibitem{b3} The Linux Foundation, ``Open Glossary of Edge Computing,'' [Online]. Available: \url{https://github.com/State-of-the-Edge/glossary}
\bibitem{b4} K. Kingsbury, ``Consistency Models,'' [Online]. Available: \url{https://jepsen.io/consistency}
\bibitem{b5} M. Bayer, ``Global Galera Database,'' [Online]. Available: \url{https://review.openstack.org/600555}
\bibitem{b6} M. M. Elbushra, J. Lindstrom, ``Causal Consistent Databases'', Open Journal of Databases (OJDB), Volume 2, Issue 1, 2015.
\end{thebibliography}
\end{document}
